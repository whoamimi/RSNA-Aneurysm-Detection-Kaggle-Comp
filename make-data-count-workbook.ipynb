{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":82370,"databundleVersionId":13015230,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Make Data Count Data Preparation\n\nDataclasses to extract dataset from `.xml` and `.pdf` for model training / tunings. \nThis also fetches meta datasets for the articles.\n","metadata":{}},{"cell_type":"code","source":"import os\n\n# Silence TF/XLA/absl chatter that spams STDERR on Kaggle\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"        # 0=all,1=INFO,2=WARNING,3=ERROR\nos.environ[\"ABSL_LOGGING_MIN_LOG_LEVEL\"] = \"3\"\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nTRAIN_Y_PATH: str = \"/kaggle/input/make-data-count-finding-data-references/train_labels.csv\"\nTRAIN_DIR_PATH:  str = \"/kaggle/input/make-data-count-finding-data-references/train\"\nTEST_DIR_PATH:  str = \"/kaggle/input/make-data-count-finding-data-references/train\"\n\nMETA_PAPER_API = \"https://api.crossref.org/works/{doi}\"\nDEFAULT_SOURCE_TYPE = 'Unknown'\nMODEL_ID = \"all-MiniLM-L6-v2\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-24T11:49:20.399579Z","iopub.execute_input":"2025-08-24T11:49:20.400148Z","iopub.status.idle":"2025-08-24T11:49:20.404581Z","shell.execute_reply.started":"2025-08-24T11:49:20.400124Z","shell.execute_reply":"2025-08-24T11:49:20.403643Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Install dependencies \n\n#!pip install -U sentence-transformers\n#!python -m sentence_transformers all-MiniLM-L6-v2\n!pip install -U pypdf\n!pip install pdfminer.six","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T11:49:20.407469Z","iopub.execute_input":"2025-08-24T11:49:20.407708Z","iopub.status.idle":"2025-08-24T11:49:26.689489Z","shell.execute_reply.started":"2025-08-24T11:49:20.407684Z","shell.execute_reply":"2025-08-24T11:49:26.688734Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (6.0.0)\nRequirement already satisfied: pdfminer.six in /usr/local/lib/python3.11/dist-packages (20250506)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.2)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (44.0.3)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Data Helpers and Utilities \n\nimport re\nimport io\nimport glob\nimport logging\nimport requests\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom pdfminer.high_level import extract_text\nfrom dataclasses import dataclass, field, asdict\nfrom typing import Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Union, Any, Optional\n\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom sentence_transformers import SentenceTransformer\n\nlogger = logging.getLogger(\"kaggle_notebook\")\nlogger.setLevel(logging.INFO)\nhandler = logging.StreamHandler()\nformatter = logging.Formatter(\n    \"%(asctime)s | %(levelname)-8s | %(message)s\", \"%Y-%m-%d %H:%M:%S\"\n)\nhandler.setFormatter(formatter)\n\ndef _read_file_binary(path: str) -> bytes:\n    with open(path, \"rb\") as f:\n        return f.read()\n\ndef _clean_ws(text: str) -> str:\n    text = re.sub(r\"\\r\\n?\", \"\\n\", text)\n    text = re.sub(r\"[ \\t]+\", \" \", text)\n    text = re.sub(r\"\\n\\s*\\n\\s*\\n+\", \"\\n\\n\", text)  # collapse >2 blank lines\n    return text.strip()\n\ndef _pdf_to_text(path: str) -> str:\n    \"\"\"Extract text from PDF using pdfminer.six if available, else PyPDF2 as fallback.\"\"\"\n    # Try pdfminer.six (best quality)\n    try:\n        # Note: extract_text opens file internally; pass path.\n        text = extract_text(path) or \"\"\n        return _clean_ws(text)\n    except Exception:\n        pass\n\n    # Fallback: PyPDF2\n    try:\n        import PyPDF2  # type: ignore\n        text_chunks: List[str] = []\n        with open(path, \"rb\") as f:\n            reader = PyPDF2.PdfReader(f)\n            for pg in reader.pages:\n                try:\n                    s = pg.extract_text() or \"\"\n                except Exception:\n                    s = \"\"\n                if s:\n                    text_chunks.append(s)\n        return _clean_ws(\"\\n\\n\".join(text_chunks))\n    except Exception:\n        return \"\"\n\ndef _xml_to_text(path: str) -> str:\n    \"\"\"Parse XML with lxml if available, else ElementTree. Extracts title/abstract/body-ish text.\"\"\"\n    xml_bytes = _read_file_binary(path)\n\n    # Try lxml first (best for namespaces/xpaths).\n    try:\n        from lxml import etree  # type: ignore\n        parser = etree.XMLParser(recover=True, huge_tree=True)\n        root = etree.fromstring(xml_bytes, parser=parser)\n\n        # Common scholarly XML patterns (JATS-ish)\n        texts: List[str] = []\n\n        # title\n        titles = root.xpath(\"//article-title|//title-group//article-title|//title\")\n        titles = [t.text if isinstance(t, etree._Element) else str(t) for t in titles]\n        titles = [t for t in titles if t]\n        if titles:\n            texts.append(\"# \" + titles[0].strip())\n\n        # abstract\n        abs_nodes = root.xpath(\"//abstract//p|//Abstract//p|//abstract\")\n        for n in abs_nodes:\n            s = \"\".join(n.itertext()) if hasattr(n, \"itertext\") else str(n)\n            s = s.strip()\n            if s:\n                texts.append(s)\n\n        # body\n        body_nodes = root.xpath(\"//body//p|//sec//p|//Body//p\")\n        for n in body_nodes:\n            s = \"\".join(n.itertext()) if hasattr(n, \"itertext\") else str(n)\n            s = s.strip()\n            if s:\n                texts.append(s)\n\n        # fallback: all text\n        if not texts:\n            all_text = \" \".join(root.itertext())\n            texts = [all_text]\n\n        return _clean_ws(\"\\n\\n\".join(texts))\n\n    except Exception:\n        # Fallback to stdlib ElementTree\n        import xml.etree.ElementTree as ET\n\n        try:\n            root = ET.fromstring(xml_bytes)\n        except Exception:\n            return \"\"  # unreadable\n\n        def itxt(el):\n            try:\n                return \"\".join(el.itertext())\n            except Exception:\n                return el.text or \"\"\n\n        # Attempt similar sections by tag name\n        parts: List[str] = []\n        # naive title\n        for tag in (\"article-title\", \"title\"):\n            for n in root.iter(tag):\n                s = (n.text or \"\").strip()\n                if s:\n                    parts.append(\"# \" + s)\n\n        # abstract\n        for tag in (\"abstract\",):\n            for n in root.iter(tag):\n                s = itxt(n).strip()\n                if s:\n                    parts.append(s)\n\n        # paragraphs\n        for tag in (\"p\",):\n            for n in root.iter(tag):\n                s = itxt(n).strip()\n                if s:\n                    parts.append(s)\n\n        if not parts:\n            parts = [itxt(root)]\n\n        return _clean_ws(\"\\n\\n\".join([p for p in parts if p]))\n\n@dataclass\nclass Author:\n    family: Optional[str] = None\n    given: Optional[str] = None\n    literal: Optional[str] = None\n\n@dataclass\nclass Issued:\n    date_parts: List[List[int]] = field(default_factory=list)\n\n@dataclass\nclass DoiResponse:\n    type: str\n    id: str\n    categories: List[str]\n    author: List[Author]\n    issued: Issued\n    abstract: str\n    DOI: str\n    publisher: str\n    title: str\n    URL: str\n    copyright: str\n\n    @staticmethod \n    def parse_response(data: Dict[str, Any]):\n        authors = [Author(**a) for a in data.get(\"author\", [])]\n        issued = Issued(date_parts=data.get(\"issued\", {}).get(\"date-parts\", []))\n        return DoiResponse(\n            type=data.get(\"type\", \"\"),\n            id=data.get(\"id\", \"\"),\n            categories=data.get(\"categories\", []),\n            author=authors,\n            issued=issued,\n            abstract=data.get(\"abstract\", \"\"),\n            DOI=data.get(\"DOI\", \"\"),\n            publisher=data.get(\"publisher\", \"\"),\n            title=data.get(\"title\", \"\"),\n            URL=data.get(\"URL\", \"\"),\n            copyright=data.get(\"copyright\", \"\"),\n        )\n\n@dataclass\nclass Article:\n    article_id: str \n    text: str \n    extension: str \n    source: str = DEFAULT_SOURCE_TYPE\n    dataset_id: str | None = None \n    dataset_id_cited: str | None = None\n    embedding: np.ndarray | None = None\n\n    @staticmethod\n    def fetch_meta_external(input_doi: str) -> dict | None:\n        url = META_PAPER_API.format(doi=input_doi)\n        \n        try:\n            r = requests.get(url)\n            return r.json()\n        except Exception as e: \n            logger.error(e)\n            return None\n\n    @staticmethod\n    def fetch_meta_doi(doi_url: str) -> DoiResponse | None:\n        try:\n            headers = {\"Accept\": \"application/vnd.citationstyles.csl+json\"}\n            r = requests.get(doi_url, headers=headers, timeout=30)\n            if r.status_code == 200:\n                result = r.json()\n                return DoiResponse.parse_response(result)\n                \n        except Exception as e: \n            logger.error(e)\n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T11:49:26.691200Z","iopub.execute_input":"2025-08-24T11:49:26.691758Z","iopub.status.idle":"2025-08-24T11:49:26.717897Z","shell.execute_reply.started":"2025-08-24T11:49:26.691723Z","shell.execute_reply":"2025-08-24T11:49:26.717031Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Dataset ID / URL Cleaners & Converters \nimport re \n\n# Test datasets that could possibly exist in the data\nsamples = [\n    {\n        \"dataset_id\": \"https://doi.org/10.1098/rspb.2016.1151\",\n        \"data\": [\"https://doi.org/10.5061/dryad.6m3n9\"],\n        \"in_text_span\": \"The data we used in this publication can be accessed from Dryad at doi:10.5061/dryad.6m3n9.\",\n        \"citation_type\": \"Primary\",\n    },\n    {\n        \"dataset_id\": \"https://doi.org/10.1098/rspb.2018.1563\",\n        \"data\": [\"https://doi.org/10.5061/dryad.c394c12\"],\n        \"in_text_span\": \"Phenotypic data and gene sequences are available from the Dryad Digital Repository: http://dx.doi.org/10.5061/dryad.c394c12\",\n        \"citation_type\": \"Primary\",\n    },\n    {\n        \"dataset_id\": \"https://doi.org/10.1534/genetics.119.302868\",\n        \"data\": [\"https://doi.org/10.25386/genetics.11365982\"],\n        \"in_text_span\": \"The authors state that all data necessary for confirming the conclusions presented in the article are represented fully within the article. Supplemental material available at figshare: https://doi.org/10.25386/genetics.11365982.\",\n        \"citation_type\": \"Primary\",\n    },\n    {\n        \"dataset_id\": \"https://doi.org/10.1038/sdata.2014.33\",\n        \"data\": [\"GSE37569\", \"GSE45042\", \"GSE28166\"],\n        \"in_text_span\": \"Primary data for Agilent and Affymetrix microarray experiments are available at the NCBI Gene Expression Omnibus (GEO, http://www.ncbi.nlm.nih.gov/geo/) under the accession numbers GSE37569, GSE45042 , GSE28166\",\n        \"citation_type\": \"Primary\",\n    },\n    {\n        \"dataset_id\": \"https://doi.org/10.12688/wellcomeopenres.15142.1\",\n        \"data\": [\"pdb 5yfp\"],\n        \"in_text_span\": \"Figure 1. Evolution and structure of the exocyst... All structural images were modelled by the authors from PDB using UCSF Chimera.\",\n        \"citation_type\": \"Secondary\",\n    },\n    {\n        \"dataset_id\": \"https://doi.org/10.3389/fimmu.2021.690817\",\n        \"data\": [\"E-MTAB-10217\", \"PRJE43395\"],\n        \"in_text_span\": \"The datasets presented in this study can be found in online repositories. The names of the repository/repositories and accession number(s) can be found below: https://www.ebi.ac.uk/arrayexpress/, E-MTAB-10217 and https://www.ebi.ac.uk/ena, PRJE43395.\",\n        \"citation_type\": \"Secondary\",\n    },\n]\n\nACCESSION_PATTERNS = [\n    # DOI (bare \"10.\" prefix, or full http(s) doi.org link, or \"doi:10...\")\n    (re.compile(r\"^(?:https?://(?:dx\\.)?doi\\.org/|doi:)?(10\\.\\d{4,9}/\\S+)$\", re.I),\n     lambda m: f\"https://doi.org/{m.group(1)}\"),\n\n    # GEO (Gene Expression Omnibus)\n    (re.compile(r\"^GSE\\d+$\", re.I),\n     lambda m: f\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={m.group(0)}\"),\n\n    # ENA run/experiment (ERR/ERS/SRR/DRR/etc.)\n    (re.compile(r\"^(ERR|ERS|SRR|SRX|SRP|DRR|DRX|DRP|ERX|ERP)\\d+$\", re.I),\n     lambda m: f\"https://www.ebi.ac.uk/ena/browser/view/{m.group(0)}\"),\n\n    # dbSNP rs IDs\n    (re.compile(r\"^rs\\d+$\", re.I),\n     lambda m: f\"https://www.ncbi.nlm.nih.gov/snp/{m.group(0)}\"),\n\n    # PDB (4-char alphanumeric IDs)\n    (re.compile(r\"^[0-9A-Za-z]{4}$\"),\n     lambda m: f\"https://www.rcsb.org/structure/{m.group(0)}\"),\n\n    # ChEMBL compounds/targets/assays\n    (re.compile(r\"^CHEMBL\\d+$\", re.I),\n     lambda m: f\"https://www.ebi.ac.uk/chembl/compound_report_card/{m.group(0)}/\"),\n\n    # DDBJ/GenBank/RefSeq nucleotide accessions (D10700, CP013147, NC_#######)\n    (re.compile(r\"^(?:[A-Z]{1,2}\\d{5,6}|NC_\\d+)$\", re.I),\n     lambda m: f\"https://www.ncbi.nlm.nih.gov/nuccore/{m.group(0)}\"),\n]\n\ndef resolve_accession(acc: str) -> Optional[str]:\n    \"\"\"Return a best-effort URL for any accession/identifier/DOI.\"\"\"\n    if acc is None or (isinstance(acc, float) and pd.isna(acc)):\n        return None\n        \n    s = str(acc).strip()\n    if not s:\n        return None\n\n    # Try regex patterns\n    for pattern, builder in ACCESSION_PATTERNS:\n        m = pattern.match(s)\n        if m:\n            return builder(m)\n\n    # Special-case string prefixes\n    if s.upper().startswith(\"ENS\"):  # Ensembl\n        return f\"https://www.ensembl.org/id/{s}\"\n    if s.upper().startswith(\"IPR\"):  # InterPro\n        return f\"https://www.ebi.ac.uk/interpro/entry/{s.upper()}\"\n    if s.upper().startswith(\"CVCL_\"):  # Cellosaurus\n        return f\"https://www.cellosaurus.org/{s.upper()}\"\n    if s.upper().startswith(\"EMPIAR-\"):  # EMPIAR\n        return f\"https://www.ebi.ac.uk/empiar/{s.upper()}\"\n    if s.upper().startswith(\"HGNC:\"):  # HGNC gene IDs\n        return f\"https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/{s.upper()}\"\n    if re.match(r\"^K\\d{5}$\", s, flags=re.I):  # KEGG Orthology\n        return f\"https://www.genome.jp/dbget-bin/www_bget?ko:{s.upper()}\"\n    if s.upper().startswith(\"EPI_ISL_\"):  # GISAID\n        return f\"https://www.gisaid.org/search?query={s}\"\n\n    # If it's already an HTTP(S) URL but didn't match DOI/PDB etc., keep as-is\n    if s.lower().startswith(\"http\"):\n        return s\n\n    # Fallback\n    return s","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T11:49:26.718808Z","iopub.execute_input":"2025-08-24T11:49:26.719065Z","iopub.status.idle":"2025-08-24T11:49:26.795770Z","shell.execute_reply.started":"2025-08-24T11:49:26.719038Z","shell.execute_reply":"2025-08-24T11:49:26.795106Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def load_train_dataset():\n    \"\"\" Loads the dataset for training. \"\"\"\n    \n    targets = pd.read_csv(TRAIN_Y_PATH)\n    logger.info(f\"Total distinct ref type Labels: {targets['type'].unique()}\")\n    \n    for path in tqdm(Path(\"/kaggle/input\").rglob(\"*\"), desc=\"Loading Train datasets\"):\n        if path.parents[1].stem == 'train' and path.is_file():\n            info = {}\n            \n            ext = path.suffix\n            \n            if ext == '.pdf': \n                text = _pdf_to_text(str(path))\n            elif ext == '.xml': \n                text = _xml_to_text(str(path))\n\n            meta = targets[targets['article_id'] == path.stem]\n            \n            info['extension'] = ext \n            info['text'] = text \n            info['article_id'] = path.stem\n            # info['embedding'] = model.encoder(info['text']) if text != '' else None\n            \n            if not meta.empty:\n                metas = meta.iloc[0].to_dict()\n                info[\"source\"] = metas.get(\"type\", DEFAULT_SOURCE_TYPE)\n                info[\"dataset_id\"] = metas.get(\"dataset_id\", None)\n                info[\"dataset_id_cited\"] = resolve_accession(info[\"dataset_id\"]) if info[\"dataset_id\"] is not None else None\n            else:\n                logger.warning(\"No metadata found for %s\", path.stem)\n\n            yield Article(**info)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T11:49:48.790844Z","iopub.execute_input":"2025-08-24T11:49:48.791308Z","iopub.status.idle":"2025-08-24T11:49:48.797438Z","shell.execute_reply.started":"2025-08-24T11:49:48.791284Z","shell.execute_reply":"2025-08-24T11:49:48.796712Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"class DoiData(Dataset): \n    \"\"\" Doi Dataset handler. \n    Target types:\n    'Unknown': missing from train dataset\n    'Missing': missing from data - predefined in the dataset\n    'Primary' / 'Secondary': Main Data Referencing Labels\n    \"\"\"\n    \n    def __init__(self):\n        self.data = list(load_train_dataset())\n        \n        assert len(self.data) > 0, \"Empty dataset loaded to instance\"\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx: int): \n        if idx > len(self.data) or idx < 0:\n            raise ValueError('Index out of range.')\n\n        # TODO: CONVERT TO X_train, y_train outputs\n        article = self.data[idx]\n        \n        if article.dataset_id_cited:\n            if meta := Article.fetch_meta_doi(article.dataset_id_cited):\n                return {**asdict(article), **asdict(meta)}\n                \n        return asdict(article)\n        \nds = DoiData()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T11:51:21.203064Z","iopub.execute_input":"2025-08-24T11:51:21.203585Z","iopub.status.idle":"2025-08-24T12:45:18.111499Z","shell.execute_reply.started":"2025-08-24T11:51:21.203559Z","shell.execute_reply":"2025-08-24T12:45:18.110894Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"Loading Train datasets: 988it [53:56,  3.28s/it] \n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"full_data = list(ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T13:00:44.256156Z","iopub.execute_input":"2025-08-24T13:00:44.256945Z","iopub.status.idle":"2025-08-24T13:10:04.936675Z","shell.execute_reply.started":"2025-08-24T13:00:44.256918Z","shell.execute_reply":"2025-08-24T13:10:04.935926Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"data = pd.DataFrame.from_records(full_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T13:19:25.396559Z","iopub.execute_input":"2025-08-24T13:19:25.396874Z","iopub.status.idle":"2025-08-24T13:19:25.405286Z","shell.execute_reply.started":"2025-08-24T13:19:25.396853Z","shell.execute_reply":"2025-08-24T13:19:25.404526Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T13:19:42.150099Z","iopub.execute_input":"2025-08-24T13:19:42.150708Z","iopub.status.idle":"2025-08-24T13:19:42.155133Z","shell.execute_reply.started":"2025-08-24T13:19:42.150683Z","shell.execute_reply":"2025-08-24T13:19:42.154451Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"(924, 18)"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"# Saving to /kaggle/working dir as parquet OR csv\ndata.to_parquet('/kaggle/working/train_dataset.parquet')\ndata.to_csv('/kaggle/working/train_dataset.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T13:20:08.645885Z","iopub.execute_input":"2025-08-24T13:20:08.646129Z","iopub.status.idle":"2025-08-24T13:20:10.718541Z","shell.execute_reply.started":"2025-08-24T13:20:08.646113Z","shell.execute_reply":"2025-08-24T13:20:10.717711Z"}},"outputs":[],"execution_count":46}]}